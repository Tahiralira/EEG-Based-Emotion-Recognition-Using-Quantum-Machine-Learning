{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (2.5.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.5.1-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting qiskit\n",
      "  Downloading qiskit-1.2.4-cp38-abi3-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pennylane in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (0.39.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: rustworkx>=0.15.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from qiskit) (0.15.1)\n",
      "Requirement already satisfied: scipy>=1.5 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from qiskit) (1.14.1)\n",
      "Collecting dill>=0.3 (from qiskit)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from qiskit) (2.9.0.post0)\n",
      "Collecting stevedore>=3.0.0 (from qiskit)\n",
      "  Downloading stevedore-5.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting symengine<0.14,>=0.11 (from qiskit)\n",
      "  Downloading symengine-0.13.0-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: autograd in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: toml in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray>=0.6.11 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (0.7.0)\n",
      "Requirement already satisfied: cachetools in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (5.5.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.39 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (0.39.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from pennylane) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
      "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit)\n",
      "  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from requests->pennylane) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from requests->pennylane) (2024.8.30)\n",
      "Downloading torchvision-0.20.1-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.5.1-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading qiskit-1.2.4-cp38-abi3-win_amd64.whl (4.6 MB)\n",
      "   ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 2.4/4.6 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.7/4.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.6/4.6 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Downloading stevedore-5.4.0-py3-none-any.whl (49 kB)\n",
      "Downloading symengine-0.13.0-cp311-cp311-win_amd64.whl (17.8 MB)\n",
      "   ---------------------------------------- 0.0/17.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/17.8 MB 10.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.4/17.8 MB 7.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 4.5/17.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 5.2/17.8 MB 6.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.6/17.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 7.3/17.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.9/17.8 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 8.4/17.8 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 8.9/17.8 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 9.2/17.8 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 9.7/17.8 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 10.0/17.8 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 10.2/17.8 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.7/17.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 11.0/17.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.5/17.8 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.1/17.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.3/17.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.8/17.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 13.1/17.8 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.6/17.8 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.9/17.8 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 14.4/17.8 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 14.7/17.8 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.2/17.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.7/17.8 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.0/17.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 16.5/17.8 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.0/17.8 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/17.8 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/17.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 17.8/17.8 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n",
      "Installing collected packages: symengine, pbr, dill, stevedore, torchvision, torchaudio, qiskit\n",
      "Successfully installed dill-0.3.9 pbr-6.1.0 qiskit-1.2.4 stevedore-5.4.0 symengine-0.13.0 torchaudio-2.5.1 torchvision-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio qiskit pennylane numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eeg', 'label'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahir\\AppData\\Local\\Temp\\ipykernel_13308\\3659042551.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Correct path using a raw string\n",
    "file_path = r'C:\\Users\\tahir\\OneDrive\\Desktop\\QC\\new_seed\\processed_eeg_data\\sample_0.pt'\n",
    "\n",
    "# Load the file\n",
    "data = torch.load(file_path)\n",
    "\n",
    "# Check the keys in the loaded dictionary\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of EEG data: <class 'torch.Tensor'>\n",
      "Shape of EEG data: torch.Size([4, 9, 9])\n",
      "Type of labels: <class 'int'>\n",
      "Content of labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahir\\AppData\\Local\\Temp\\ipykernel_13308\\4001245854.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Correct path using a raw string\n",
    "file_path = r'C:\\Users\\tahir\\OneDrive\\Desktop\\QC\\new_seed\\processed_eeg_data\\sample_0.pt'\n",
    "\n",
    "# Load the file\n",
    "data = torch.load(file_path)\n",
    "\n",
    "# Accessing the EEG and label data\n",
    "eeg_data = data['eeg']\n",
    "labels = data['label']\n",
    "\n",
    "# Check the type and content\n",
    "print(\"Type of EEG data:\", type(eeg_data))\n",
    "print(\"Shape of EEG data:\", eeg_data.shape if isinstance(eeg_data, torch.Tensor) else \"Not a Tensor\")\n",
    "print(\"Type of labels:\", type(labels))\n",
    "print(\"Content of labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahir\\AppData\\Local\\Temp\\ipykernel_13308\\763456621.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2745, 4, 9, 9])\n",
      "torch.Size([2745])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "directory = r'C:\\Users\\tahir\\OneDrive\\Desktop\\QC\\new_seed\\processed_eeg_data'\n",
    "file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pt')]\n",
    "\n",
    "all_eeg_data = []\n",
    "all_labels = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    data = torch.load(file_path)\n",
    "    all_eeg_data.append(data['eeg'])\n",
    "    all_labels.append(data['label'])\n",
    "\n",
    "# Convert list to tensor if necessary\n",
    "all_eeg_data = torch.stack(all_eeg_data)\n",
    "all_labels = torch.tensor(all_labels)\n",
    "\n",
    "print(all_eeg_data.shape)\n",
    "print(all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping tensors.\n",
    "    \n",
    "    Arguments:\n",
    "        data (Tensor): contains the EEG data points.\n",
    "        targets (Tensor): contains all of the labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        assert data.size(0) == targets.size(0)  # Ensure data and targets are matched by first dimension\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahir\\AppData\\Local\\Temp\\ipykernel_13308\\2922797028.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(directory):\n",
    "    file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pt')]\n",
    "    all_eeg_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        data = torch.load(file_path)\n",
    "        all_eeg_data.append(data['eeg'])\n",
    "        all_labels.append(data['label'])\n",
    "\n",
    "    all_eeg_data = torch.stack(all_eeg_data)\n",
    "    all_labels = torch.tensor(all_labels)\n",
    "    return all_eeg_data, all_labels\n",
    "\n",
    "# Load data\n",
    "directory = r'C:\\Users\\tahir\\OneDrive\\Desktop\\QC\\new_seed\\processed_eeg_data'\n",
    "eeg_data, labels = load_data(directory)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming eeg_data and labels are already loaded and are PyTorch tensors\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from the model: tensor([[-1.1967, -1.3913, -0.8006],\n",
      "        [-1.6341, -1.2578, -0.6528],\n",
      "        [-1.3440, -1.4449, -0.6863],\n",
      "        [-1.0529, -1.3980, -0.9063],\n",
      "        [-1.3506, -1.3199, -0.7471],\n",
      "        [-1.3806, -1.2880, -0.7492],\n",
      "        [-1.0740, -1.3959, -0.8898],\n",
      "        [-1.4806, -1.2971, -0.6948],\n",
      "        [-1.2154, -1.3966, -0.7853],\n",
      "        [-1.1627, -1.3314, -0.8598]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Set up a quantum device\n",
    "num_qubits = 4\n",
    "dev = qml.device('default.qubit', wires=num_qubits)\n",
    "\n",
    "# Define a quantum circuit with PyTorch interface\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def circuit(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(num_qubits))\n",
    "    qml.templates.BasicEntanglerLayers(weights, wires=range(num_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]\n",
    "\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(3, num_qubits))  # 3 layers, num_qubits qubits\n",
    "        \n",
    "        # Setup the device and the QNode\n",
    "        self.dev = qml.device('default.qubit', wires=num_qubits)\n",
    "        @qml.qnode(self.dev, interface='torch')\n",
    "        def qnode(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=range(num_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(num_qubits))\n",
    "            return [qml.expval(qml.PauliZ(wire)) for wire in range(num_qubits)]\n",
    "        \n",
    "        self.qnode = qnode\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert each input to a tensor with proper dtype before applying the quantum node\n",
    "        quantum_outputs = [torch.tensor(self.qnode(x[i], self.weights)).float() for i in range(x.size(0))]\n",
    "        # Stack all outputs into a single tensor\n",
    "        return torch.stack(quantum_outputs)\n",
    "\n",
    "\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.pre_quantum = nn.Linear(4*9*9, num_qubits)  # Input dimension to match flattened EEG data\n",
    "        self.quantum_layer = QuantumLayer()\n",
    "        self.post_quantum = nn.Linear(num_qubits, 3)  # Output for 3 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the EEG data\n",
    "        x = self.pre_quantum(x)  # Pre-quantum linear transformation\n",
    "        x = self.quantum_layer(x)  # Quantum layer processing\n",
    "        x = self.post_quantum(x)  # Post-quantum linear transformation\n",
    "        return torch.log_softmax(x, dim=1)  # Log-softmax for classification\n",
    "\n",
    "# Example to create the model and print its architecture\n",
    "# Initialize the model\n",
    "model = HybridModel()\n",
    "\n",
    "# Dummy input to check the forward pass\n",
    "dummy_input = torch.randn(10, 4, 9, 9)  # Batch size of 10, each with shape (4, 9, 9)\n",
    "\n",
    "# Forward pass through the model to check for errors\n",
    "try:\n",
    "    output = model(dummy_input)\n",
    "    print(\"Output from the model:\", output)\n",
    "except Exception as e:\n",
    "    print(\"Error during model forward pass:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Display CUDA version and the current GPU device\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 24.3.1 from c:\\Users\\tahir\\OneDrive\\Desktop\\QC\\.venv\\Lib\\site-packages\\pip (python 3.11)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tahir\\onedrive\\desktop\\qc\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#CUDA 12.4\n",
    "\n",
    "%pip install --verbose torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.03487519841156917\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for idx in range(0, X_train.size(0), 32):  # Batch size of 32\n",
    "        batch_X, batch_y = X_train[idx:idx+32], y_train[idx:idx+32]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X.float())\n",
    "        loss = loss_func(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / (idx+1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.0911, Validation Loss: 1.0784\n",
      "Saved Best Model\n",
      "Epoch 2, Train Loss: 1.0906, Validation Loss: 1.0780\n",
      "Saved Best Model\n",
      "Epoch 3, Train Loss: 1.0900, Validation Loss: 1.0773\n",
      "Saved Best Model\n",
      "Epoch 4, Train Loss: 1.0900, Validation Loss: 1.0769\n",
      "Saved Best Model\n",
      "Epoch 5, Train Loss: 1.0896, Validation Loss: 1.0760\n",
      "Saved Best Model\n",
      "Epoch 6, Train Loss: 1.0895, Validation Loss: 1.0760\n",
      "Saved Best Model\n",
      "Epoch 7, Train Loss: 1.0893, Validation Loss: 1.0758\n",
      "Saved Best Model\n",
      "Epoch 8, Train Loss: 1.0887, Validation Loss: 1.0755\n",
      "Saved Best Model\n",
      "Epoch 9, Train Loss: 1.0887, Validation Loss: 1.0751\n",
      "Saved Best Model\n",
      "Epoch 10, Train Loss: 1.0884, Validation Loss: 1.0748\n",
      "Saved Best Model\n",
      "Epoch 11, Train Loss: 1.0883, Validation Loss: 1.0745\n",
      "Saved Best Model\n",
      "Epoch 12, Train Loss: 1.0879, Validation Loss: 1.0743\n",
      "Saved Best Model\n",
      "Epoch 13, Train Loss: 1.0879, Validation Loss: 1.0737\n",
      "Saved Best Model\n",
      "Epoch 14, Train Loss: 1.0873, Validation Loss: 1.0734\n",
      "Saved Best Model\n",
      "Epoch 15, Train Loss: 1.0873, Validation Loss: 1.0732\n",
      "Saved Best Model\n",
      "Epoch 16, Train Loss: 1.0872, Validation Loss: 1.0728\n",
      "Saved Best Model\n",
      "Epoch 17, Train Loss: 1.0870, Validation Loss: 1.0727\n",
      "Saved Best Model\n",
      "Epoch 18, Train Loss: 1.0865, Validation Loss: 1.0727\n",
      "Epoch 19, Train Loss: 1.0865, Validation Loss: 1.0722\n",
      "Saved Best Model\n",
      "Epoch 20, Train Loss: 1.0858, Validation Loss: 1.0721\n",
      "Saved Best Model\n",
      "Epoch 21, Train Loss: 1.0859, Validation Loss: 1.0715\n",
      "Saved Best Model\n",
      "Epoch 22, Train Loss: 1.0856, Validation Loss: 1.0714\n",
      "Saved Best Model\n",
      "Epoch 23, Train Loss: 1.0855, Validation Loss: 1.0713\n",
      "Saved Best Model\n",
      "Epoch 24, Train Loss: 1.0854, Validation Loss: 1.0711\n",
      "Saved Best Model\n",
      "Epoch 25, Train Loss: 1.0854, Validation Loss: 1.0709\n",
      "Saved Best Model\n",
      "Epoch 26, Train Loss: 1.0852, Validation Loss: 1.0707\n",
      "Saved Best Model\n",
      "Epoch 27, Train Loss: 1.0848, Validation Loss: 1.0709\n",
      "Epoch 28, Train Loss: 1.0847, Validation Loss: 1.0705\n",
      "Saved Best Model\n",
      "Epoch 29, Train Loss: 1.0846, Validation Loss: 1.0700\n",
      "Saved Best Model\n",
      "Epoch 30, Train Loss: 1.0844, Validation Loss: 1.0704\n",
      "Epoch 31, Train Loss: 1.0842, Validation Loss: 1.0695\n",
      "Saved Best Model\n",
      "Epoch 32, Train Loss: 1.0841, Validation Loss: 1.0697\n",
      "Epoch 33, Train Loss: 1.0842, Validation Loss: 1.0689\n",
      "Saved Best Model\n",
      "Epoch 34, Train Loss: 1.0836, Validation Loss: 1.0691\n",
      "Epoch 35, Train Loss: 1.0834, Validation Loss: 1.0691\n",
      "Epoch 36, Train Loss: 1.0833, Validation Loss: 1.0691\n",
      "Epoch 37, Train Loss: 1.0833, Validation Loss: 1.0689\n",
      "Epoch 38, Train Loss: 1.0829, Validation Loss: 1.0685\n",
      "Saved Best Model\n",
      "Epoch 39, Train Loss: 1.0825, Validation Loss: 1.0684\n",
      "Saved Best Model\n",
      "Epoch 40, Train Loss: 1.0826, Validation Loss: 1.0684\n",
      "Epoch 41, Train Loss: 1.0825, Validation Loss: 1.0679\n",
      "Saved Best Model\n",
      "Epoch 42, Train Loss: 1.0820, Validation Loss: 1.0677\n",
      "Saved Best Model\n",
      "Epoch 43, Train Loss: 1.0821, Validation Loss: 1.0678\n",
      "Epoch 44, Train Loss: 1.0819, Validation Loss: 1.0674\n",
      "Saved Best Model\n",
      "Epoch 45, Train Loss: 1.0817, Validation Loss: 1.0674\n",
      "Saved Best Model\n",
      "Epoch 46, Train Loss: 1.0816, Validation Loss: 1.0673\n",
      "Saved Best Model\n",
      "Epoch 47, Train Loss: 1.0814, Validation Loss: 1.0668\n",
      "Saved Best Model\n",
      "Epoch 48, Train Loss: 1.0812, Validation Loss: 1.0670\n",
      "Epoch 49, Train Loss: 1.0813, Validation Loss: 1.0669\n",
      "Epoch 50, Train Loss: 1.0809, Validation Loss: 1.0667\n",
      "Saved Best Model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model, train_loader, and test_loader are already defined\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, device='cuda'):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = validate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early stopping and saving the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Saved Best Model\")\n",
    "\n",
    "def validate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss = criterion(output, target)\n",
    "            total_val_loss += val_loss.item()\n",
    "    return total_val_loss / len(test_loader)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tahir\\AppData\\Local\\Temp\\ipykernel_13308\\2955481243.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 45.90%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
